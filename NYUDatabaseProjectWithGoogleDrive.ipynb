{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBwsXt3kIRjL"
      },
      "source": [
        "This is the notebook that I used to generate my classification model on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7_Py8iWI2U3",
        "outputId": "32bee965-bf5b-4826-e5f1-5b2a77f85625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-8nTzJWs6wfe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image, image_dataset_from_directory\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 # MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5fwPXqHnhE1m"
      },
      "outputs": [],
      "source": [
        "# For reproducability\n",
        "seed = 12222022\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define base_directory where Alzheimer_s Dataset is located\n",
        "base_directory = 'Alzheimer_s Dataset'\n",
        "base_directory = '/content/drive/MyDrive/' + base_directory #for google drive"
      ],
      "metadata": {
        "id": "fjWEr69qWb7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL0wr7wWiBj8"
      },
      "outputs": [],
      "source": [
        "#code from guide to define data sets, modified for Drive\n",
        "image_generator = ImageDataGenerator(rescale=1/255, validation_split=0)\n",
        "train_ds = image_generator.flow_from_directory(batch_size=16,\n",
        "                                                 directory= base_directory+'/train',\n",
        "                                                 shuffle=True,\n",
        "                                                 target_size=(224,224), \n",
        "                                                 subset=\"training\",\n",
        "                                                 class_mode='categorical')\n",
        "image_generator = ImageDataGenerator(rescale=1/255,validation_split=0.2) \n",
        "val_ds = image_generator.flow_from_directory(batch_size=16,\n",
        "                                                 directory= base_directory+'/test',\n",
        "                                                 shuffle=True,\n",
        "                                                 target_size=(224,224),\n",
        "                                                 class_mode='categorical')\n",
        "\n",
        "#I prefer my own code\n",
        "# batch_size = 32\n",
        "# img_height = 224\n",
        "# img_width = 224\n",
        "\n",
        "# train_dir = \"/content/drive/MyDrive/Alzheimer_s Dataset/train\" \n",
        "# test_dir = \"/content/drive/MyDrive/Alzheimer_s Dataset/test\"\n",
        "\n",
        "# train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "#     train_dir,\n",
        "#     validation_split=0.1,  \n",
        "#     subset='training', \n",
        "#     shuffle=True,\n",
        "#     seed = seed,\n",
        "#     image_size=(img_height, img_width),\n",
        "#     batch_size=batch_size,\n",
        "#     color_mode=\"rgb\",\n",
        "# )\n",
        "\n",
        "# val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "#     test_dir,\n",
        "#     validation_split=0.9,\n",
        "#     subset=\"validation\",\n",
        "#     shuffle=True,\n",
        "#     seed = seed,\n",
        "#     image_size=(img_height, img_width),\n",
        "#     batch_size=batch_size,\n",
        "#     color_mode=\"rgb\",\n",
        "# )\n",
        "\n",
        "# class_names = np.array(train_ds.class_names)\n",
        "# num_classes = len(class_names)\n",
        "# print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDXEYAuln9zj"
      },
      "outputs": [],
      "source": [
        "# #From one of my previous projects, code I found online did not inlude obvious buffer optimizations \n",
        "# data_augmentation = tf.keras.Sequential([\n",
        "#   # tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "#   # tf.keras.layers.RandomRotation(0.02),\n",
        "#   # tf.keras.layers.RandomContrast(factor=0.2),\n",
        "#   # tf.keras.layers.RandomFlip(mode=\"horizontal\"),\n",
        "#   # tf.keras.layers.RandomRotation(factor=0.05),\n",
        "#   #tf.keras.layers.RandomZoom(height_factor=(-0.1, -0)),\n",
        "#   #tf.keras.layers.RandomContrast(factor=0.05),\n",
        "# ])\n",
        "\n",
        "# AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# def aug(ds):\n",
        "#   # Use data augmentation only on the training set.\n",
        "#   ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
        "#   # Use buffered prefetching on all datasets.\n",
        "#   return ds\n",
        "\n",
        "# # train_ds = aug(train_ds)\n",
        "\n",
        "# normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "# train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
        "# val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
        "\n",
        "# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# for image_batch, labels_batch in train_ds:\n",
        "#   print(image_batch.shape)\n",
        "#   print(labels_batch.shape)\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fv8-_wOod-V"
      },
      "outputs": [],
      "source": [
        "# #Defining the model with summary of structure\n",
        "# METRICS = [\n",
        "#     keras.metrics.AUC(name='auc'),\n",
        "#     \"acc\",\n",
        "# ]\n",
        "\n",
        "# base_model = MobileNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "\n",
        "# base_model.trainable = False\n",
        "\n",
        "# inputs = tf.keras.Input(shape=(img_height, img_width, 3))\n",
        "\n",
        "# #x = base_model.layers[-1].output \n",
        "# #x = base_model.output\n",
        "# x = Flatten()(base_model.output)\n",
        "\n",
        "# #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# #convolutional layer before final layer\n",
        "# x = tf.keras.layers.Dense(32, activation = \"relu\")(x)\n",
        "\n",
        "# #predictions = tf.keras.layers.Dense(4, activation= \"softmax\")(x)\n",
        "# predictions = tf.keras.layers.Dense(num_classes)(x)\n",
        "# model = Model(inputs = base_model.input, outputs = predictions)\n",
        "# #model = tf.keras.Model(inputs, outputs = predictions)\n",
        "\n",
        "# #model = base_model\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVsgR138rvOa"
      },
      "outputs": [],
      "source": [
        "# #Training the model\n",
        "# base_learning_rate = 1e-2\n",
        "# model.compile(\n",
        "#   optimizer=tf.keras.optimizers.Adam(\n",
        "#        learning_rate=base_learning_rate\n",
        "#   ),\n",
        "#   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#   metrics=METRICS,  \n",
        "# )\n",
        "\n",
        "# #another attempt to avoid overfitting\n",
        "# earlycallback = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor =\"val_loss\", \n",
        "#     #monitor = \"val_acc\",\n",
        "#     min_delta = 0.0005,\n",
        "#     mode =\"auto\", patience = 3,\n",
        "#     verbose = True,\n",
        "#     restore_best_weights = True\n",
        "# )\n",
        "\n",
        "# NUM_EPOCHS = 1000\n",
        "\n",
        "# history = model.fit(train_ds, validation_data=val_ds, epochs=NUM_EPOCHS, callbacks=earlycallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5oC2DL8wnET",
        "outputId": "d714f804-5ea0-4b79-f8a5-9aad916d0cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "321/321 [==============================] - 2615s 8s/step - loss: 2.7130 - auc: 0.8281 - acc: 0.6407 - val_loss: 3.3432 - val_auc: 0.7897 - val_acc: 0.5825\n",
            "Epoch 2/50\n",
            "321/321 [==============================] - 18s 56ms/step - loss: 1.0141 - auc: 0.9392 - acc: 0.8266 - val_loss: 4.8442 - val_auc: 0.7562 - val_acc: 0.5747\n",
            "Epoch 3/50\n",
            "321/321 [==============================] - 18s 55ms/step - loss: 0.5803 - auc: 0.9682 - acc: 0.8875 - val_loss: 4.1594 - val_auc: 0.8100 - val_acc: 0.6458\n",
            "Epoch 4/50\n",
            "321/321 [==============================] - 17s 53ms/step - loss: 0.2703 - auc: 0.9870 - acc: 0.9320 - val_loss: 2.4739 - val_auc: 0.8537 - val_acc: 0.6904\n",
            "Epoch 5/50\n",
            "321/321 [==============================] - 18s 55ms/step - loss: 0.4821 - auc: 0.9738 - acc: 0.9098 - val_loss: 8.8059 - val_auc: 0.7226 - val_acc: 0.5504\n",
            "Epoch 6/50\n",
            "321/321 [==============================] - 17s 53ms/step - loss: 0.2717 - auc: 0.9868 - acc: 0.9473 - val_loss: 5.1924 - val_auc: 0.7736 - val_acc: 0.6005\n",
            "Epoch 7/50\n",
            "321/321 [==============================] - 17s 53ms/step - loss: 0.3816 - auc: 0.9810 - acc: 0.9383 - val_loss: 4.2838 - val_auc: 0.8182 - val_acc: 0.6701\n",
            "Epoch 8/50\n",
            "321/321 [==============================] - 17s 53ms/step - loss: 0.4792 - auc: 0.9764 - acc: 0.9274 - val_loss: 10.8431 - val_auc: 0.7392 - val_acc: 0.5856\n",
            "Epoch 9/50\n",
            "321/321 [==============================] - 17s 54ms/step - loss: 0.3396 - auc: 0.9838 - acc: 0.9504 - val_loss: 8.2910 - val_auc: 0.7830 - val_acc: 0.6411\n",
            "Epoch 10/50\n",
            "321/321 [==============================] - 17s 54ms/step - loss: 0.2062 - auc: 0.9907 - acc: 0.9639 - val_loss: 11.6198 - val_auc: 0.7319 - val_acc: 0.5754\n",
            "Epoch 11/50\n",
            "321/321 [==============================] - 17s 54ms/step - loss: 0.3134 - auc: 0.9854 - acc: 0.9561 - val_loss: 10.0305 - val_auc: 0.7629 - val_acc: 0.6145\n",
            "Epoch 12/50\n",
            "321/321 [==============================] - 17s 54ms/step - loss: 0.3646 - auc: 0.9830 - acc: 0.9518 - val_loss: 7.8926 - val_auc: 0.8001 - val_acc: 0.6575\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4d846cf460>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "base_model = MobileNetV2(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "x = Flatten()(base_model.output)\n",
        "\n",
        "prediction = Dense(4, activation='softmax')(x)\n",
        "\n",
        "modelvgg = Model(inputs=base_model.input, outputs=prediction)\n",
        "\n",
        "modelvgg.compile(optimizer='adam',\n",
        "loss=tf.losses.CategoricalCrossentropy(),\n",
        "metrics=[keras.metrics.AUC(name='auc'),'acc'])\n",
        "callback = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                            patience=8,\n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "modelvgg.fit(train_ds, epochs=50, validation_data=val_ds, callbacks=callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ1tN1mErTp-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc2uByCMmMbq"
      },
      "source": [
        "For saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LE-j36vwmJPA"
      },
      "outputs": [],
      "source": [
        "export_path = \"models/output\"\n",
        "modelvgg.save(export_path+'AlzheimersMRI.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For testing the model on single images"
      ],
      "metadata": {
        "id": "nFn4P-32WpYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
        "\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "v4nAvCpDH0dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#picking from train to confirm classes notation\n",
        "filename = base_directory+\"/train/ModerateDemented/moderateDem18.jpg\"\n",
        "img = load_img(filename , color_mode=\"rgb\",target_size = (224 , 224))\n",
        "img = img_to_array(img)\n",
        "img = img.reshape(1 , 224 ,224 ,3)\n",
        "img = img.astype('float32')\n",
        "img = img/255.0\n",
        "result = model.predict(img)"
      ],
      "metadata": {
        "id": "xpILdeejWtCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4738LXfmWz-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}